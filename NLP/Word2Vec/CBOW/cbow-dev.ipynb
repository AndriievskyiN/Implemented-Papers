{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Data"]},{"cell_type":"markdown","metadata":{},"source":["The model was trained on the BBC News Articles dataset.\n","\n","You can download the dataset here:\n","\n","https://www.kaggle.com/datasets/pariza/bbc-news-summary"]},{"cell_type":"markdown","metadata":{"id":"6JoFe3uDNGip"},"source":["# Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:58:43.156196Z","iopub.status.busy":"2023-08-18T05:58:43.155716Z","iopub.status.idle":"2023-08-18T05:58:49.499286Z","shell.execute_reply":"2023-08-18T05:58:49.498304Z","shell.execute_reply.started":"2023-08-18T05:58:43.156158Z"},"id":"1ZW82-ZuNGiy","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/andriievskyi/miniforge3/envs/pytorchenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as opt\n","import pandas as pd\n","from torch.nn import functional as F\n","from transformers import BertTokenizer\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","import os\n","import random"]},{"cell_type":"markdown","metadata":{"id":"Ps2T7cHnNGiy"},"source":["# Hyperparameters"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:58:49.502068Z","iopub.status.busy":"2023-08-18T05:58:49.501394Z","iopub.status.idle":"2023-08-18T05:58:49.508089Z","shell.execute_reply":"2023-08-18T05:58:49.506955Z","shell.execute_reply.started":"2023-08-18T05:58:49.502030Z"},"id":"-PdOgEkQNGiy","trusted":true},"outputs":[],"source":["BATCH_SIZE = 64\n","CONTEXT_SIZE = 8\n","DATA_PATH = \"../data/Articles\""]},{"cell_type":"markdown","metadata":{"id":"e_feH6r4NGiy"},"source":["# Load data"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:58:49.510073Z","iopub.status.busy":"2023-08-18T05:58:49.509739Z","iopub.status.idle":"2023-08-18T05:58:49.522841Z","shell.execute_reply":"2023-08-18T05:58:49.521934Z","shell.execute_reply.started":"2023-08-18T05:58:49.510045Z"},"id":"sdrf4dhuNGiz","trusted":true},"outputs":[],"source":["def get_data(data_path):\n","    data = {'File Name': [], 'Text': []}\n","\n","    for folder_name in os.listdir(data_path):\n","        folder_path = os.path.join(data_path, folder_name)\n","        if os.path.isdir(folder_path):\n","            for file_name in os.listdir(folder_path):\n","                if file_name.endswith('.txt'):\n","                    file_path = os.path.join(folder_path, file_name)\n","                    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n","                        content = file.read()\n","                        data['File Name'].append(file_name)\n","                        data['Text'].append(content)\n","\n","    df = pd.DataFrame(data)\n","\n","    # Remove the first line (title) from the 'Text' column\n","    df['Text'] = df['Text'].str.split('\\n', 1).str[1]\n","\n","    # Remove extra spaces and symbols\n","    df['Text'] = df['Text'].str.replace(r'\\n', ' ', regex=True)\n","    df['Text'] = df['Text'].str.replace(r'[^\\w\\s]', ' ', regex=True)  # Remove non-alphanumeric characters\n","    df['Text'] = df['Text'].str.replace(r'\\s+', ' ', regex=True)  # Remove extra spaces\n","\n","    # Remove numbers\n","    df['Text'] = df['Text'].str.replace(r'\\d+', '', regex=True)\n","\n","    # Lowercase all words\n","    df['Text'] = df['Text'].str.lower()\n","\n","    return pd.DataFrame(df[\"Text\"])"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:58:49.527483Z","iopub.status.busy":"2023-08-18T05:58:49.527225Z","iopub.status.idle":"2023-08-18T05:58:59.094927Z","shell.execute_reply":"2023-08-18T05:58:59.093895Z","shell.execute_reply.started":"2023-08-18T05:58:49.527460Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>musicians groups are to tackle us visa regula...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>u who have won three prestigious grammy award...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>rock singer pete doherty has been involved in...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>the film adaptation of lemony snicket novels ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ocean s twelve the crime caper sequel starrin...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                Text\n","0   musicians groups are to tackle us visa regula...\n","1   u who have won three prestigious grammy award...\n","2   rock singer pete doherty has been involved in...\n","3   the film adaptation of lemony snicket novels ...\n","4   ocean s twelve the crime caper sequel starrin..."]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["data = get_data(DATA_PATH)\n","data.head()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:58:59.096978Z","iopub.status.busy":"2023-08-18T05:58:59.096311Z","iopub.status.idle":"2023-08-18T05:59:00.025527Z","shell.execute_reply":"2023-08-18T05:59:00.023199Z","shell.execute_reply.started":"2023-08-18T05:58:59.096940Z"},"trusted":true},"outputs":[],"source":["# Create a CountVectorizer instance\n","tokenizer = CountVectorizer(lowercase=True)\n","\n","# Fit the vectorizer on your text data\n","text_data = data['Text'].tolist()  # Assuming your DataFrame is named 'df'\n","tokenizer.fit(text_data)\n","\n","# Get the vocabulary (list of words) and its corresponding indices\n","vocabulary = tokenizer.get_feature_names_out()\n","vocabulary = np.append(vocabulary, \"UNK\")\n","word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n","idx_to_word = {idx: word for idx, word in enumerate(vocabulary)}\n","\n","# Example: Transform a text into a vector representation\n","text_vector = tokenizer.transform(['example']).toarray()"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"execution":{"iopub.execute_input":"2023-08-18T05:59:00.029923Z","iopub.status.busy":"2023-08-18T05:59:00.029425Z","iopub.status.idle":"2023-08-18T05:59:01.919423Z","shell.execute_reply":"2023-08-18T05:59:01.918338Z","shell.execute_reply.started":"2023-08-18T05:59:00.029870Z"},"id":"VnYDhc7NNGiz","outputId":"9d11d0ae-9d96-4dac-b1a6-9b027eb0f955","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>windows</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>musicians groups are to tackle us visa regulat...</td>\n","      <td>which</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>groups are to tackle us visa regulations which...</td>\n","      <td>are</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>are to tackle us visa regulations which are fo...</td>\n","      <td>blamed</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>to tackle us visa regulations which are blamed...</td>\n","      <td>for</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>tackle us visa regulations which are blamed fo...</td>\n","      <td>hindering</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             windows     labels\n","0  musicians groups are to tackle us visa regulat...      which\n","1  groups are to tackle us visa regulations which...        are\n","2  are to tackle us visa regulations which are fo...     blamed\n","3  to tackle us visa regulations which are blamed...        for\n","4  tackle us visa regulations which are blamed fo...  hindering"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["def create_windows_dataframe(data, context_size):\n","    all_windows_str = []  # Modified to store windows as strings\n","    all_labels = []\n","\n","    for index, row in data.iterrows():\n","        input_sequence = row[\"Text\"].split()\n","\n","        num_windows = len(input_sequence) - 2 * context_size\n","\n","        for i in range(num_windows):\n","            window = input_sequence[i: i + context_size] + input_sequence[i + context_size + 1: i + 2 * context_size + 1]\n","            window_str = \" \".join(window)  # Convert the window list to a string\n","            label = input_sequence[i + context_size]\n","            all_windows_str.append(window_str)  # Append the window string\n","            all_labels.append(label)\n","\n","    # Create a pandas DataFrame from the lists\n","    windows_df = pd.DataFrame({\n","        'windows': all_windows_str,  # Use the modified list containing window strings\n","        'labels': all_labels\n","    })\n","\n","    return windows_df\n","\n","# Assuming you have a DataFrame named 'test_df' with a column named 'Text'\n","windows_dataframe = create_windows_dataframe(data, CONTEXT_SIZE)\n","windows_dataframe.head()"]},{"cell_type":"code","execution_count":113,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.921257Z","iopub.status.busy":"2023-08-18T05:59:01.920809Z","iopub.status.idle":"2023-08-18T05:59:01.926855Z","shell.execute_reply":"2023-08-18T05:59:01.925610Z","shell.execute_reply.started":"2023-08-18T05:59:01.921223Z"},"id":"zA30KHV3NGi0","trusted":true},"outputs":[],"source":["# Split the data into train and val\n","X, y = windows_dataframe[\"windows\"], windows_dataframe[\"labels\"]\n","# X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8)\n","\n","# X_train = X_train.to_list()\n","# X_val = X_val.to_list()\n","# y_train = y_train.to_list()\n","# y_val = y_val.to_list()"]},{"cell_type":"markdown","metadata":{"id":"2zPkpY4UNGi0"},"source":["# Creating a Text Dataset"]},{"cell_type":"code","execution_count":114,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.928811Z","iopub.status.busy":"2023-08-18T05:59:01.928433Z","iopub.status.idle":"2023-08-18T05:59:01.939608Z","shell.execute_reply":"2023-08-18T05:59:01.938541Z","shell.execute_reply.started":"2023-08-18T05:59:01.928764Z"},"id":"GcJDCeigNGi0","trusted":true},"outputs":[],"source":["class TextDataset(Dataset):\n","    def __init__(self, windows, labels, word_to_idx, unk_token):\n","        self.windows = windows\n","        self.labels = labels\n","        self.word_to_idx = word_to_idx\n","        self.unk_token = unk_token\n","\n","    def __len__(self):\n","        return len(self.windows)\n","\n","    def __getitem__(self, index):\n","        window = self.windows[index]\n","        label = self.labels[index]\n","\n","        # Tokenize the text\n","        tokenized_window = torch.tensor([self.word_to_idx.get(word, self.unk_token) for word in window.split()])\n","        tokenized_label = torch.tensor([self.word_to_idx.get(label, self.unk_token)])\n","\n","\n","        return {\n","            \"windows\": tokenized_window,\n","            \"labels\": tokenized_label\n","        }"]},{"cell_type":"code","execution_count":115,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.942709Z","iopub.status.busy":"2023-08-18T05:59:01.941981Z","iopub.status.idle":"2023-08-18T05:59:01.954721Z","shell.execute_reply":"2023-08-18T05:59:01.953729Z","shell.execute_reply.started":"2023-08-18T05:59:01.942675Z"},"trusted":true},"outputs":[],"source":["unk_token = word_to_idx[\"UNK\"]"]},{"cell_type":"code","execution_count":116,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.960316Z","iopub.status.busy":"2023-08-18T05:59:01.959603Z","iopub.status.idle":"2023-08-18T05:59:01.967112Z","shell.execute_reply":"2023-08-18T05:59:01.966103Z","shell.execute_reply.started":"2023-08-18T05:59:01.960283Z"},"id":"FivdAEUmNGi0","trusted":true},"outputs":[],"source":["train_ds = TextDataset(X, y, word_to_idx, unk_token)\n","# val_ds = TextDataset(X_val, y_val, word_to_idx, unk_token)"]},{"cell_type":"code","execution_count":117,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.968877Z","iopub.status.busy":"2023-08-18T05:59:01.968433Z","iopub.status.idle":"2023-08-18T05:59:01.979871Z","shell.execute_reply":"2023-08-18T05:59:01.978899Z","shell.execute_reply.started":"2023-08-18T05:59:01.968844Z"},"id":"ihVx7BLDNGi0","trusted":true},"outputs":[],"source":["# Create data loaders\n","train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n","# val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"3OPn9ld7NGi0"},"source":["# Define the model architecture"]},{"cell_type":"code","execution_count":118,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.981640Z","iopub.status.busy":"2023-08-18T05:59:01.981275Z","iopub.status.idle":"2023-08-18T05:59:01.990739Z","shell.execute_reply":"2023-08-18T05:59:01.989804Z","shell.execute_reply.started":"2023-08-18T05:59:01.981609Z"},"id":"2qs-qibRNGi0","trusted":true},"outputs":[],"source":["class CBOW(nn.Module):\n","    def __init__(self, vocab_size, d_model, hidden_dim):\n","        super(CBOW, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, d_model)\n","        self.fc1 = nn.Linear(d_model, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x):\n","        x = torch.mean(self.embedding(x), dim=1) # (batch_size, d_model)\n","        x = self.fc1(x) # (batch_size, hidden_dim)\n","        x = self.fc2(x) # (batch_size, vocab_size)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"eCTAG6oLNGi0"},"source":["# Traning the model\n"]},{"cell_type":"code","execution_count":119,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:01.993001Z","iopub.status.busy":"2023-08-18T05:59:01.992569Z","iopub.status.idle":"2023-08-18T05:59:02.068774Z","shell.execute_reply":"2023-08-18T05:59:02.067685Z","shell.execute_reply.started":"2023-08-18T05:59:01.992954Z"},"id":"uT0-Yb59NGi0","trusted":true},"outputs":[],"source":["VOCAB_SIZE = len(vocabulary)\n","D_MODEL = 512\n","HIDDEN_DIM = 128\n","N_EPOCHS = 10\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":120,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T05:59:02.073356Z","iopub.status.busy":"2023-08-18T05:59:02.072994Z","iopub.status.idle":"2023-08-18T05:59:05.646791Z","shell.execute_reply":"2023-08-18T05:59:05.644948Z","shell.execute_reply.started":"2023-08-18T05:59:02.073326Z"},"id":"uIdrQ7_rNGi1","trusted":true},"outputs":[],"source":["model = CBOW(VOCAB_SIZE, D_MODEL, HIDDEN_DIM).to(DEVICE)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T07:01:15.395532Z","iopub.status.busy":"2023-08-18T07:01:15.394825Z","iopub.status.idle":"2023-08-18T07:01:15.402560Z","shell.execute_reply":"2023-08-18T07:01:15.401573Z","shell.execute_reply.started":"2023-08-18T07:01:15.395498Z"},"id":"6F0wHT29NGi1","trusted":true},"outputs":[],"source":["# Define loss fucntion and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = opt.Adam(model.parameters(), lr=0.000001)"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T07:01:15.731446Z","iopub.status.busy":"2023-08-18T07:01:15.731096Z","iopub.status.idle":"2023-08-18T07:01:15.740775Z","shell.execute_reply":"2023-08-18T07:01:15.739670Z","shell.execute_reply.started":"2023-08-18T07:01:15.731420Z"},"id":"l1JQgC3eNGi1","trusted":true},"outputs":[],"source":["# Training loop\n","def train(n_epochs, model=model, train_loader=train_loader, tokenizer=tokenizer, criterion=criterion, optimizer=optimizer):\n","    for epoch in range(n_epochs):\n","        total_loss = 0\n","\n","        model.train()\n","        for batch in tqdm(train_loader, desc=f\"EPOCH: {epoch+1} / {n_epochs}\", leave=False):\n","            # Get data from the loader and put it on GPU if available.\n","            windows = batch[\"windows\"].to(DEVICE)\n","            labels = batch[\"labels\"].to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            outputs = model(windows)\n","\n","            loss = criterion(outputs, labels.view(-1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-08-18T07:01:16.122559Z","iopub.status.busy":"2023-08-18T07:01:16.122246Z","iopub.status.idle":"2023-08-18T07:10:30.699965Z","shell.execute_reply":"2023-08-18T07:10:30.698303Z","shell.execute_reply.started":"2023-08-18T07:01:16.122532Z"},"id":"QsixxUpCNGi1","outputId":"14fa00fc-37c2-45b6-a934-1cc0f2b229a9","trusted":true},"outputs":[],"source":["train(N_EPOCHS)"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T06:59:20.161060Z","iopub.status.busy":"2023-08-18T06:59:20.160690Z","iopub.status.idle":"2023-08-18T06:59:20.171972Z","shell.execute_reply":"2023-08-18T06:59:20.171048Z","shell.execute_reply.started":"2023-08-18T06:59:20.161028Z"},"trusted":true},"outputs":[],"source":["def get_analogy(word1, word2, word3, model=model, word_to_idx=word_to_idx, idx_to_word=idx_to_word, unk_token=word_to_idx[\"UNK\"], n=5):\n","    word1_index = word_to_idx.get(word1, unk_token)\n","    word2_index = word_to_idx.get(word2, unk_token)\n","    word3_index = word_to_idx.get(word3, unk_token)\n","\n","    # Access the embedding layer of your model\n","    embedding_layer = model.embedding  # Replace with the actual name of your embedding layer\n","\n","    # Get the embedding vector for the word\n","    word1_emb = embedding_layer.weight[word1_index]\n","    word2_emb = embedding_layer.weight[word2_index]\n","    word3_emb = embedding_layer.weight[word3_index]\n","\n","    analogy_vector = word1_emb - word2_emb + word3_emb\n","    analogy_vector_cpu = analogy_vector.cpu().detach()\n","    word_embeddings_cpu = embedding_layer.weight.cpu().detach()\n","\n","    # Calculate cosine similarity between the analogy vector and all word embeddings\n","    similarity_scores = cosine_similarity(analogy_vector_cpu.reshape(1, -1), word_embeddings_cpu)\n","    \n","    # Find the indices of the n most similar words\n","    most_similar_indices = np.argsort(similarity_scores[0])[-n:][::-1]\n","\n","    # Get the words associated with the most similar indices and their similarity scores\n","    similar_words = [idx_to_word[idx] for idx in most_similar_indices]\n","    similar_scores = [similarity_scores[0][idx] for idx in most_similar_indices]\n","\n","    for word, score in zip(similar_words, similar_scores):\n","        print(f\"Word: {word}, Cosine Similarity: {score:.4f}\")"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T06:59:20.398762Z","iopub.status.busy":"2023-08-18T06:59:20.398484Z","iopub.status.idle":"2023-08-18T06:59:20.516737Z","shell.execute_reply":"2023-08-18T06:59:20.515774Z","shell.execute_reply.started":"2023-08-18T06:59:20.398737Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Word: king, Cosine Similarity: 0.6175\n","Word: woman, Cosine Similarity: 0.5853\n","Word: academically, Cosine Similarity: 0.2030\n","Word: arabia, Cosine Similarity: 0.1982\n","Word: burns, Cosine Similarity: 0.1971\n"]}],"source":["get_analogy(\"king\", \"man\", \"woman\") # Doesn't really understand "]},{"cell_type":"code","execution_count":122,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T06:59:25.806541Z","iopub.status.busy":"2023-08-18T06:59:25.806119Z","iopub.status.idle":"2023-08-18T06:59:25.820671Z","shell.execute_reply":"2023-08-18T06:59:25.819529Z","shell.execute_reply.started":"2023-08-18T06:59:25.806509Z"},"trusted":true},"outputs":[],"source":["def check_similarity(word1, word2, model=model, word_to_idx=word_to_idx, idx_to_word=idx_to_word, unk_token=word_to_idx[\"UNK\"]):\n","    word1_index = word_to_idx.get(word1, unk_token)\n","    word2_index = word_to_idx.get(word2, unk_token)\n","\n","    # Access the embedding layer of your model\n","    embedding_layer = model.embedding\n","\n","    # Get the embedding vectors for the words\n","    word1_emb = embedding_layer.weight[word1_index]\n","    word2_emb = embedding_layer.weight[word2_index]\n","\n","    # Calculate cosine similarity between the embedding vectors\n","    similarity_score = cosine_similarity(word1_emb.cpu().detach().reshape(1, -1), word2_emb.cpu().detach().reshape(1, -1))\n","\n","    print(f\"Are '{word1}' and '{word2}' similar?\")\n","    print(f\"Cosine Similarity: {similarity_score[0][0]:.4f}\")"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-08-18T07:00:39.818289Z","iopub.status.busy":"2023-08-18T07:00:39.817908Z","iopub.status.idle":"2023-08-18T07:00:39.826377Z","shell.execute_reply":"2023-08-18T07:00:39.825157Z","shell.execute_reply.started":"2023-08-18T07:00:39.818261Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Are 'tall' and 'short' similar?\n","Cosine Similarity: -0.0156\n"]}],"source":["check_similarity(\"tall\", \"short\") # Kind of understands that these are opposite (negative score)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_most_similar_words(input_word, model=model, word_to_idx=word_to_idx, idx_to_word=idx_to_word, unk_token=word_to_idx[\"UNK\"], n=5):\n","    word_index = word_to_idx.get(input_word, unk_token)\n","\n","    # Access the embedding layer of your model\n","    embedding_layer = model.embedding\n","\n","    # Get the embedding vector for the input word\n","    input_word_emb = embedding_layer.weight[word_index]\n","\n","    # Calculate cosine similarity between the embedding vector of the input word and all word embeddings\n","    similarity_scores = cosine_similarity(input_word_emb.cpu().detach().reshape(1, -1), embedding_layer.weight.cpu().detach())\n","\n","    # Find the indices of the n most similar words\n","    most_similar_indices = np.argsort(similarity_scores[0])[-n:][::-1]\n","\n","    # Get the words associated with the most similar indices and their similarity scores\n","    similar_words = [idx_to_word[idx] for idx in most_similar_indices]\n","    similar_scores = [similarity_scores[0][idx] for idx in most_similar_indices]\n","\n","    print(f\"Most Similar Words to '{input_word}':\")\n","    for word, score in zip(similar_words, similar_scores):\n","        print(f\"Word: {word}, Cosine Similarity: {score:.4f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":4}
