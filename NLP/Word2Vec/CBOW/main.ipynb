{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "from datasets import load_dataset\n",
    "import nltk\n",
    "from tqdm.auto import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/andriievskyi/.cache/huggingface/datasets/wikitext/wikitext-2-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00, 71.67it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36718/36718 [00:00<00:00, 73197.85it/s]\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "\n",
    "for i in tqdm(raw_dataset[\"train\"]):\n",
    "    if i[\"text\"]:\n",
    "        text.append(i[\"text\"].replace(\"\\n\", \" \").strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['= Valkyria Chronicles III =',\n",
       " 'Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" .',\n",
       " \"The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n .\",\n",
       " \"It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 .\",\n",
       " '= = Gameplay = =']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the numbers\n",
    "for i, txt in enumerate(text):\n",
    "    text[i] = re.sub(\"\\d\", \" \", txt)\n",
    "    text[i] = re.sub(\"[ ]+\", \" \", txt).lower()\n",
    "\n",
    "raw_text = \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' valkyria chronicles iii  senjō no valkyria 3  unrecorded chronicles  japanese  戦場のヴァルキュリア3  lit  va'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "punctuation = \"()-=+!@#$%^&*.,\\|'\\\"<>§`~{}[];:\"\n",
    "\n",
    "no_punc = \"\"\n",
    "for char in raw_text:\n",
    "    if char not in punctuation:\n",
    "        no_punc += char\n",
    "\n",
    "no_punc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andriievskyi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andriievskyi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from string import punctuation\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess(text):\n",
    "    # Tokenize text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    tokens = [token for token in tokens if token not in punctuation]\n",
    "    \n",
    "    # Remove stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Stem tokens\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    # Return preprocessed text\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = preprocess(no_punc)\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('first', 4248), ('one', 4001), ('–', 3934), ('also', 3842), ('two', 3566), ('time', 3370), ('year', 3120), ('use', 3067), ('game', 2936), ('state', 2894)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def most_frequent_words(text, n=10):\n",
    "    # split the text into words\n",
    "    words = text\n",
    "\n",
    "    # count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # return the top n most frequent words\n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "# example usage\n",
    "top_words = most_frequent_words(text, n=10)\n",
    "print(top_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the frequency of each word in the text\n",
    "word_counts = Counter(text)\n",
    "\n",
    "# Filter out words that appear less than 3 times\n",
    "vocab = set([word for word in word_counts.keys() if word_counts[word] >= 3])\n",
    "\n",
    "# Add the \"<unk>\" token to the vocabulary\n",
    "vocab.add(\"<unk>\")\n",
    "\n",
    "# Assign integer indices to the words in the vocabulary\n",
    "stoi = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Create a reverse mapping from integer indices to words\n",
    "itos = {i: word for word, i in stoi.items()}\n",
    "\n",
    "# Define functions to encode and decode text using the vocabulary\n",
    "encode = lambda text: [stoi.get(word, stoi[\"<unk>\"]) for word in text]\n",
    "decode = lambda indices: [itos[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21102, 13968]\n",
      "['test', 'line']\n"
     ]
    }
   ],
   "source": [
    "# Try the encoder and decoder\n",
    "test_line = \"test line\".split()\n",
    "print(encode(test_line))\n",
    "print(decode(encode(test_line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1044592]) torch.int64\n",
      "tensor([12706, 16560,  8157,  6402, 12706, 10882, 19476, 16560,  5522,  6317,\n",
      "         8354, 12706,  4085, 10882,  6540, 11767, 12706, 16560,  8157,  2797,\n",
      "         1654,  7580,  9652,  5840,  6123, 12847, 18611, 19625,  3779,  4756,\n",
      "         6328,  6465,  6015,  2731,  1654,  6670, 12847, 12706,  1728, 15491,\n",
      "         8919,  7580,  8324,  7821, 15813,  9369, 11612,  5136,  2452,  9881,\n",
      "        12847, 13868, 17166,   892,  4116, 13633,  8196, 18898,  3125, 15507,\n",
      "        13739,  4969,  1256, 18791, 17157,  9998, 14515, 12171, 13633,  6545,\n",
      "        11808, 12847,  3133, 18611,  3152,  7822, 15266, 20981,   850, 10057,\n",
      "        12706, 16560,  5718,  8801,   499, 16982,  1728, 21429, 19759, 10338,\n",
      "         8498,  5093, 12847,  2601,  1728, 16957, 13621, 12470,  6545,  7168])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12706, 16560,  8157,  ...,  7581, 16827,  4384])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([835673]) torch.Size([208919])\n"
     ]
    }
   ],
   "source": [
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "print(train_data.shape, val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12706, 16560,  8157,  6402, 12706, 10882, 19476, 16560,  5522,  6317,\n",
       "         8354, 12706])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 5 # context length before and after the target word\n",
    "\n",
    "x = torch.cat((train_data[:C], train_data[C+1:2*C+1]), dim=0)\n",
    "y = train_data[C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When the input is tensor([12706, 16560,  8157,  6402, 12706, 19476, 16560,  5522,  6317,  8354]): \n",
      "the label is: 10882\n",
      "\n",
      "When the input is valkyria chronicl iii senjō valkyria unrecord chronicl japanes 戦場のヴァルキュリア3 lit \n",
      " The output is 3\n",
      "\n",
      "When the input is tensor([16560,  8157,  6402, 12706, 10882, 16560,  5522,  6317,  8354, 12706]): \n",
      "the label is: 19476\n",
      "\n",
      "When the input is chronicl iii senjō valkyria 3 chronicl japanes 戦場のヴァルキュリア3 lit valkyria \n",
      " The output is unrecord\n",
      "\n",
      "When the input is tensor([ 8157,  6402, 12706, 10882, 19476,  5522,  6317,  8354, 12706,  4085]): \n",
      "the label is: 16560\n",
      "\n",
      "When the input is iii senjō valkyria 3 unrecord japanes 戦場のヴァルキュリア3 lit valkyria battlefield \n",
      " The output is chronicl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_data[:8])-C):\n",
    "    x = torch.cat((train_data[i:C+i], train_data[C+i+1:2*C+1+i]), dim=0)\n",
    "    y = train_data[C+i]\n",
    "\n",
    "    print(f\"When the input is {x}: \\nthe label is: {y}\\n\")\n",
    "    x_str = \" \".join(decode(x.numpy()))\n",
    "    y_str = decode([y.tolist()])[0]\n",
    "\n",
    "    print(f\"When the input is {x_str} \\n The output is {y_str}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xb.shape = torch.Size([16, 10])\n",
      "yb.shape = torch.Size([16])\n",
      "\n",
      "inputs:\n",
      "tensor([[ 2654, 13654, 16719, 12682,  3087, 14121,  3591, 21434,  3087, 13921],\n",
      "        [18365, 18931,   212,   782,  4883,   189,  6923,  2182,   189,  3530],\n",
      "        [ 1644, 18789,  6964, 11436,  8927,  6737, 12260, 18789, 17283, 15095],\n",
      "        [ 3460,  2462,  9881, 21429,  8675, 10622, 13142, 14192, 12119, 18173],\n",
      "        [19934, 15007, 19813, 12371, 18918,  9020, 20330, 11234, 10161,  2351],\n",
      "        [13621,  4897,  3262, 19787, 19170,  3289,  9652, 15507, 12371, 20422],\n",
      "        [20254,   438, 18710,  7371,  3087, 14564,  7943,  5281,  7742,  9638],\n",
      "        [12412,  4357,  9097,  7331,    28, 12452,  5461,  2349, 18459,  2176],\n",
      "        [15507,  8178, 19396, 18026, 19634,  9524,  7313, 17825,  8204, 18026],\n",
      "        [ 5081,  3987, 12385, 16909, 21429, 10489,  3503, 17798, 18638, 20918],\n",
      "        [20644,  6545, 19442, 16535,  8873, 20644, 11549,  3627, 15211,  2530],\n",
      "        [ 3032,   850, 13678,  1672, 20669, 21379,  9578, 12026,  6869,  2360],\n",
      "        [17869,   949,   850,  3255,  3201,  6545,  2223, 18255, 16272, 18537],\n",
      "        [17969,  6567, 21358, 19653, 19374,  4815, 16486, 14865,  4415, 18256],\n",
      "        [ 2140,   850, 16630,  4361, 11828,  8944, 15275, 14843, 14790,   535],\n",
      "        [ 7243, 14121, 18953, 16362, 14121, 11834, 18179,  2269,  9266, 11713]])\n",
      "targets:\n",
      "tensor([10969, 10434, 13151,  9881,  6142, 20485, 20881,  5093,   474, 12170,\n",
      "        18123,  5249,  6545, 10526,  3087, 12825])\n",
      "---------\n",
      "When the context is [2654, 13654, 16719, 12682, 3087, 14121, 3591, 21434, 3087, 13921], the target is 10969\n",
      "When the context is [18365, 18931, 212, 782, 4883, 189, 6923, 2182, 189, 3530], the target is 10434\n",
      "When the context is [1644, 18789, 6964, 11436, 8927, 6737, 12260, 18789, 17283, 15095], the target is 13151\n",
      "When the context is [3460, 2462, 9881, 21429, 8675, 10622, 13142, 14192, 12119, 18173], the target is 9881\n",
      "When the context is [19934, 15007, 19813, 12371, 18918, 9020, 20330, 11234, 10161, 2351], the target is 6142\n",
      "When the context is [13621, 4897, 3262, 19787, 19170, 3289, 9652, 15507, 12371, 20422], the target is 20485\n",
      "When the context is [20254, 438, 18710, 7371, 3087, 14564, 7943, 5281, 7742, 9638], the target is 20881\n",
      "When the context is [12412, 4357, 9097, 7331, 28, 12452, 5461, 2349, 18459, 2176], the target is 5093\n",
      "When the context is [15507, 8178, 19396, 18026, 19634, 9524, 7313, 17825, 8204, 18026], the target is 474\n",
      "When the context is [5081, 3987, 12385, 16909, 21429, 10489, 3503, 17798, 18638, 20918], the target is 12170\n",
      "When the context is [20644, 6545, 19442, 16535, 8873, 20644, 11549, 3627, 15211, 2530], the target is 18123\n",
      "When the context is [3032, 850, 13678, 1672, 20669, 21379, 9578, 12026, 6869, 2360], the target is 5249\n",
      "When the context is [17869, 949, 850, 3255, 3201, 6545, 2223, 18255, 16272, 18537], the target is 6545\n",
      "When the context is [17969, 6567, 21358, 19653, 19374, 4815, 16486, 14865, 4415, 18256], the target is 10526\n",
      "When the context is [2140, 850, 16630, 4361, 11828, 8944, 15275, 14843, 14790, 535], the target is 3087\n",
      "When the context is [7243, 14121, 18953, 16362, 14121, 11834, 18179, 2269, 9266, 11713], the target is 12825\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "def get_batch(batch_size: int = 16, C:int = 5, split: str = \"train\"):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "\n",
    "    ix = torch.randint(len(data) -  2 * C, (batch_size, ))\n",
    "    x = torch.stack([\n",
    "        torch.cat((\n",
    "            data[i:C+i], data[C+i+1:2*C+1+i]), dim=0) \n",
    "                for i in ix.tolist()])\n",
    "\n",
    "    y = torch.stack([data[C+i] for i in ix.tolist()])\n",
    "\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch()\n",
    "print(f\"{xb.shape = }\")\n",
    "print(f\"{yb.shape = }\")\n",
    "\n",
    "print(\"\\ninputs:\")\n",
    "print(xb)\n",
    "print(\"targets:\")\n",
    "print(yb)\n",
    "\n",
    "print(\"---------\")\n",
    "for i in range(16):\n",
    "    context = xb[i]\n",
    "    target = yb[i]\n",
    "\n",
    "    print(f\"When the context is {context.tolist()}, the target is {target}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, embed_size, vocab_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, 128)\n",
    "        self.activation1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "        self.activation2 = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # B * (C*2)\n",
    "        embeds = self.embedding(inputs)\n",
    "\n",
    "        batch, context, embed = embeds.shape\n",
    "\n",
    "        embeds = torch.sum(embeds, dim=1)\n",
    "        out = self.fc1(embeds)\n",
    "        out = self.activation1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.activation2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_word_embedding(self, word):\n",
    "        word = torch.tensor([stoi[word]])\n",
    "        return self.embedding(word).view(1, -1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = CBOW(128, len(vocab))\n",
    "loss = nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(cbow.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "tensor([ 3730, 16895, 20485, 17431, 19129,  2396,  6545,  8589, 14368, 20725,\n        13920, 15473,  5613,  6313, 17969,  6545])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/andriievskyi/Desktop/Coding_and_ML/implemented_papers/word2vec/CBOW/main.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andriievskyi/Desktop/Coding_and_ML/implemented_papers/word2vec/CBOW/main.ipynb#Y145sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m xb, yb \u001b[39m=\u001b[39m get_batch()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andriievskyi/Desktop/Coding_and_ML/implemented_papers/word2vec/CBOW/main.ipynb#Y145sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m log_probs \u001b[39m=\u001b[39m cbow(xb)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/andriievskyi/Desktop/Coding_and_ML/implemented_papers/word2vec/CBOW/main.ipynb#Y145sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss(log_probs, torch\u001b[39m.\u001b[39mtensor([stoi[yb]]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/andriievskyi/Desktop/Coding_and_ML/implemented_papers/word2vec/CBOW/main.ipynb#Y145sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Backprop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andriievskyi/Desktop/Coding_and_ML/implemented_papers/word2vec/CBOW/main.ipynb#Y145sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyError\u001b[0m: tensor([ 3730, 16895, 20485, 17431, 19129,  2396,  6545,  8589, 14368, 20725,\n        13920, 15473,  5613,  6313, 17969,  6545])"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "\n",
    "    xb, yb = get_batch()\n",
    "    \n",
    "    log_probs = cbow(xb)\n",
    "    total_loss += loss(log_probs, torch.tensor([stoi[yb]]))\n",
    "\n",
    "    # Backprop\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b648088feacc2269b01156dbc8717337e6120979ab5058beb24ccafdd1242407"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
