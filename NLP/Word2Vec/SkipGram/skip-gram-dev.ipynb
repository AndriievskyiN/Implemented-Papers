{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import pandas as pd\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "CONTEXT_SIZE = 5\n",
    "DATA_PATH = \"../data/Articles\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(data_path):\n",
    "    data = {'File Name': [], 'Text': []}\n",
    "\n",
    "    for folder_name in os.listdir(data_path):\n",
    "        folder_path = os.path.join(data_path, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for file_name in os.listdir(folder_path):\n",
    "                if file_name.endswith('.txt'):\n",
    "                    file_path = os.path.join(folder_path, file_name)\n",
    "                    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "                        content = file.read()\n",
    "                        data['File Name'].append(file_name)\n",
    "                        data['Text'].append(content)\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Remove the first line (title) from the 'Text' column\n",
    "    df['Text'] = df['Text'].str.split('\\n', 1).str[1]\n",
    "\n",
    "    # Remove extra spaces and symbols\n",
    "    df['Text'] = df['Text'].str.replace(r'\\n', ' ', regex=True)\n",
    "    df['Text'] = df['Text'].str.replace(r'[^\\w\\s]', ' ', regex=True)  # Remove non-alphanumeric characters\n",
    "    df['Text'] = df['Text'].str.replace(r'\\s+', ' ', regex=True)  # Remove extra spaces\n",
    "\n",
    "    # Remove numbers\n",
    "    df['Text'] = df['Text'].str.replace(r'\\d+', '', regex=True)\n",
    "\n",
    "    # Lowercase all words\n",
    "    df['Text'] = df['Text'].str.lower()\n",
    "\n",
    "    return pd.DataFrame(df[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>musicians groups are to tackle us visa regula...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u who have won three prestigious grammy award...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rock singer pete doherty has been involved in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the film adaptation of lemony snicket novels ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve the crime caper sequel starrin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0   musicians groups are to tackle us visa regula...\n",
       "1   u who have won three prestigious grammy award...\n",
       "2   rock singer pete doherty has been involved in...\n",
       "3   the film adaptation of lemony snicket novels ...\n",
       "4   ocean s twelve the crime caper sequel starrin..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data(DATA_PATH)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pairs(data, context_size):\n",
    "    pairs = []\n",
    "    for _, row in data.iterrows():\n",
    "        input_sequence = row[\"Text\"].split()\n",
    "\n",
    "        num_windows = len(input_sequence) - 2 * context_size\n",
    "\n",
    "        for i in range(num_windows):\n",
    "            window = input_sequence[i: i + context_size] + input_sequence[i + context_size + 1: i + 2 * context_size + 1]\n",
    "            target = input_sequence[i + context_size]\n",
    "            \n",
    "            for word in window:\n",
    "                pairs.append([target, word])\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['us', 'musicians'],\n",
       " ['us', 'groups'],\n",
       " ['us', 'are'],\n",
       " ['us', 'to'],\n",
       " ['us', 'tackle']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = create_pairs(data, CONTEXT_SIZE)\n",
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer instance\n",
    "tokenizer = CountVectorizer(lowercase=True)\n",
    "\n",
    "# Fit the vectorizer on your text data\n",
    "text_data = data['Text'].tolist()  # Assuming your DataFrame is named 'df'\n",
    "tokenizer.fit(text_data)\n",
    "\n",
    "# Get the vocabulary (list of words) and its corresponding indices\n",
    "vocabulary = tokenizer.get_feature_names_out()\n",
    "vocabulary = np.append(vocabulary, \"UNK\")\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "# Example: Transform a text into a vector representation\n",
    "text_vector = tokenizer.transform(['example']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_token = word_to_idx[\"UNK\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Text Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, pairs, word_to_idx, unk_token):\n",
    "        self.pairs = pairs\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target = self.pairs[index][0]\n",
    "        context = self.pairs[index][1]\n",
    "\n",
    "        # Tokenize the text\n",
    "        target = torch.tensor([self.word_to_idx.get(target, self.unk_token)])\n",
    "        context = torch.tensor([self.word_to_idx.get(context, self.unk_token)])\n",
    "\n",
    "        return {\n",
    "            \"target\": target,\n",
    "            \"context\": context\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextDataset(pairs, word_to_idx, unk_token)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out = self.out(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocabulary)\n",
    "D_MODEL = 512\n",
    "N_EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(VOCAB_SIZE, D_MODEL).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss fucntion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = opt.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train(n_epochs, model=model, train_loader=train_loader, tokenizer=tokenizer, criterion=criterion, optimizer=optimizer):\n",
    "    for epoch in range(n_epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader, desc=f\"EPOCH: {epoch+1} / {n_epochs}\", leave=False):\n",
    "            # Get data from the loader and put it on GPU if available.\n",
    "            target = batch[\"target\"].to(DEVICE)\n",
    "            context = batch[\"context\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(target)\n",
    "\n",
    "            loss = criterion(outputs.view(-1, VOCAB_SIZE), context.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs} | Loss: {avg_loss:.4f}\")\n",
    "        rand_idx = torch.randint(0, len(batch), (1, )).item()\n",
    "        target = idx_to_word.get(target[rand_idx].item())\n",
    "        context = idx_to_word.get(context[rand_idx].item())\n",
    "\n",
    "        pred_probs = torch.softmax(outputs[rand_idx], dim=1)\n",
    "        pred_token = torch.argmax(pred_probs)\n",
    "        predicted_context = idx_to_word.get(pred_token.item())\n",
    "\n",
    "        print(f\"TARGET: {target} | CONTEXT: {context} | PREDICTED: {predicted_context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
